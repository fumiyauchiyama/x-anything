0: INFO    25-01-19 22:41:22.943370 - 0:00:03 - Epoch 0 started
0: INFO    25-01-19 22:41:27.012692 - 0:00:07 - Epoch 0, iter 0, loss: 39.16058349609375
0: INFO    25-01-19 22:41:27.840452 - 0:00:08 - Epoch 0, iter 1, loss: 38.458988189697266
0: INFO    25-01-19 22:41:28.406283 - 0:00:08 - Epoch 0, iter 2, loss: 38.470706939697266
0: INFO    25-01-19 22:41:28.938620 - 0:00:09 - Epoch 0, iter 3, loss: 37.83191680908203
0: INFO    25-01-19 22:41:29.461519 - 0:00:09 - Epoch 0, iter 4, loss: 38.97296905517578
0: INFO    25-01-19 22:41:30.015851 - 0:00:10 - Epoch 0, iter 5, loss: 37.57474899291992
0: INFO    25-01-19 22:41:30.594655 - 0:00:10 - Epoch 0, iter 6, loss: 33.98358154296875
0: INFO    25-01-19 22:41:31.125847 - 0:00:11 - Epoch 0, iter 7, loss: 36.02842712402344
0: INFO    25-01-19 22:41:31.691392 - 0:00:12 - Epoch 0, iter 8, loss: 35.114131927490234
0: INFO    25-01-19 22:41:32.228298 - 0:00:12 - Epoch 0, iter 9, loss: 34.747318267822266
0: INFO    25-01-19 22:41:32.782113 - 0:00:13 - Epoch 0, iter 10, loss: 32.60551834106445
0: INFO    25-01-19 22:41:33.332713 - 0:00:13 - Epoch 0, iter 11, loss: 32.88312911987305
0: INFO    25-01-19 22:41:33.870913 - 0:00:14 - Epoch 0, iter 12, loss: 31.761905670166016
0: INFO    25-01-19 22:41:34.418126 - 0:00:14 - Epoch 0, iter 13, loss: 30.36567497253418
0: INFO    25-01-19 22:41:34.960854 - 0:00:15 - Epoch 0, iter 14, loss: 27.352628707885742
0: INFO    25-01-19 22:41:35.512647 - 0:00:15 - Epoch 0, iter 15, loss: 26.297893524169922
0: INFO    25-01-19 22:41:36.067876 - 0:00:16 - Epoch 0, iter 16, loss: 23.480960845947266
0: INFO    25-01-19 22:41:36.609822 - 0:00:16 - Epoch 0, iter 17, loss: 23.205039978027344
0: INFO    25-01-19 22:41:37.172270 - 0:00:17 - Epoch 0, iter 18, loss: 21.219146728515625
0: INFO    25-01-19 22:41:37.695368 - 0:00:18 - Epoch 0, iter 19, loss: 20.268272399902344
0: INFO    25-01-19 22:41:38.244540 - 0:00:18 - Epoch 0, iter 20, loss: 18.639949798583984
0: INFO    25-01-19 22:41:38.792379 - 0:00:19 - Epoch 0, iter 21, loss: 17.49985694885254
0: INFO    25-01-19 22:41:39.460347 - 0:00:19 - Epoch 0, iter 22, loss: 16.68053436279297
0: INFO    25-01-19 22:41:39.982633 - 0:00:20 - Epoch 0, iter 23, loss: 15.385074615478516
0: INFO    25-01-19 22:41:40.528047 - 0:00:20 - Epoch 0, iter 24, loss: 14.735316276550293
0: INFO    25-01-19 22:41:41.074114 - 0:00:21 - Epoch 0, iter 25, loss: 15.434823989868164
0: INFO    25-01-19 22:41:41.625923 - 0:00:22 - Epoch 0, iter 26, loss: 13.25997543334961
0: INFO    25-01-19 22:41:42.158452 - 0:00:22 - Epoch 0, iter 27, loss: 13.264004707336426
0: INFO    25-01-19 22:41:42.702391 - 0:00:23 - Epoch 0, iter 28, loss: 12.638235092163086
0: INFO    25-01-19 22:41:43.231235 - 0:00:23 - Epoch 0, iter 29, loss: 12.214346885681152
0: INFO    25-01-19 22:41:43.797556 - 0:00:24 - Epoch 0, iter 30, loss: 12.000802993774414
0: INFO    25-01-19 22:41:44.346457 - 0:00:24 - Epoch 0, iter 31, loss: 11.801352500915527
0: INFO    25-01-19 22:41:44.884200 - 0:00:25 - Epoch 0, iter 32, loss: 11.728586196899414
0: INFO    25-01-19 22:41:45.414685 - 0:00:25 - Epoch 0, iter 33, loss: 11.904781341552734
0: INFO    25-01-19 22:41:45.963303 - 0:00:26 - Epoch 0, iter 34, loss: 21.165803909301758
0: INFO    25-01-19 22:41:46.503679 - 0:00:26 - Epoch 0, iter 35, loss: 11.728766441345215
0: INFO    25-01-19 22:41:47.041332 - 0:00:27 - Epoch 0, iter 36, loss: 11.540245056152344
0: INFO    25-01-19 22:41:47.575512 - 0:00:27 - Epoch 0, iter 37, loss: 11.289653778076172
0: INFO    25-01-19 22:41:48.108798 - 0:00:28 - Epoch 0, iter 38, loss: 11.287463188171387
0: INFO    25-01-19 22:41:48.663288 - 0:00:29 - Epoch 0, iter 39, loss: 11.399820327758789
0: INFO    25-01-19 22:41:49.201366 - 0:00:29 - Epoch 0, iter 40, loss: 11.606329917907715
0: INFO    25-01-19 22:41:49.745056 - 0:00:30 - Epoch 0, iter 41, loss: 11.594307899475098
0: INFO    25-01-19 22:41:50.296854 - 0:00:30 - Epoch 0, iter 42, loss: 11.683677673339844
0: INFO    25-01-19 22:41:50.839740 - 0:00:31 - Epoch 0, iter 43, loss: 11.477338790893555
0: INFO    25-01-19 22:41:51.380390 - 0:00:31 - Epoch 0, iter 44, loss: 11.12933349609375
0: INFO    25-01-19 22:41:51.923454 - 0:00:32 - Epoch 0, iter 45, loss: 11.18791675567627
0: INFO    25-01-19 22:41:52.462825 - 0:00:32 - Epoch 0, iter 46, loss: 24.438247680664062
0: INFO    25-01-19 22:41:52.992508 - 0:00:33 - Epoch 0, iter 47, loss: 11.16584300994873
0: INFO    25-01-19 22:41:53.538975 - 0:00:33 - Epoch 0, iter 48, loss: 11.147967338562012
0: INFO    25-01-19 22:41:54.091926 - 0:00:34 - Epoch 0, iter 49, loss: 11.490325927734375
0: INFO    25-01-19 22:41:54.632281 - 0:00:35 - Epoch 0, iter 50, loss: 19.603933334350586
0: INFO    25-01-19 22:41:55.167247 - 0:00:35 - Epoch 0, iter 51, loss: 12.402153015136719
0: INFO    25-01-19 22:41:55.705886 - 0:00:36 - Epoch 0, iter 52, loss: 17.008024215698242
0: INFO    25-01-19 22:41:56.244447 - 0:00:36 - Epoch 0, iter 53, loss: 11.519657135009766
0: INFO    25-01-19 22:41:56.889232 - 0:00:37 - Epoch 0, iter 54, loss: 11.766453742980957
0: INFO    25-01-19 22:41:57.393041 - 0:00:37 - Epoch 0, iter 55, loss: 12.032166481018066
0: INFO    25-01-19 22:41:57.958345 - 0:00:38 - Epoch 0, iter 56, loss: 11.963850975036621
0: INFO    25-01-19 22:41:58.504183 - 0:00:38 - Epoch 0, iter 57, loss: 13.321035385131836
0: INFO    25-01-19 22:41:59.043731 - 0:00:39 - Epoch 0, iter 58, loss: 15.466743469238281
0: INFO    25-01-19 22:41:59.569661 - 0:00:39 - Epoch 0, iter 59, loss: 11.988524436950684
0: INFO    25-01-19 22:42:00.124232 - 0:00:40 - Epoch 0, iter 60, loss: 12.053335189819336
0: INFO    25-01-19 22:42:00.672328 - 0:00:41 - Epoch 0, iter 61, loss: 12.115859985351562
0: INFO    25-01-19 22:42:01.209939 - 0:00:41 - Epoch 0, iter 62, loss: 14.44491958618164
0: INFO    25-01-19 22:42:01.751759 - 0:00:42 - Epoch 0, iter 63, loss: 11.70165729522705
0: INFO    25-01-19 22:42:02.334676 - 0:00:42 - Epoch 0, iter 64, loss: 11.542880058288574
0: INFO    25-01-19 22:42:02.896980 - 0:00:43 - Epoch 0, iter 65, loss: 11.518221855163574
0: INFO    25-01-19 22:42:03.423258 - 0:00:43 - Epoch 0, iter 66, loss: 11.422242164611816
0: INFO    25-01-19 22:42:03.968702 - 0:00:44 - Epoch 0, iter 67, loss: 11.310846328735352
0: INFO    25-01-19 22:42:04.512806 - 0:00:44 - Epoch 0, iter 68, loss: 11.33443832397461
0: INFO    25-01-19 22:42:05.048873 - 0:00:45 - Epoch 0, iter 69, loss: 11.531475067138672
0: INFO    25-01-19 22:42:05.598598 - 0:00:45 - Epoch 0, iter 70, loss: 14.696556091308594
0: INFO    25-01-19 22:42:06.168551 - 0:00:46 - Epoch 0, iter 71, loss: 12.151453971862793
0: INFO    25-01-19 22:42:06.905972 - 0:00:47 - Epoch 0, iter 72, loss: 11.156777381896973
0: INFO    25-01-19 22:42:07.420932 - 0:00:47 - Epoch 0, iter 73, loss: 21.0390682220459
0: INFO    25-01-19 22:42:07.961299 - 0:00:48 - Epoch 0, iter 74, loss: 11.213323593139648
0: INFO    25-01-19 22:42:08.500771 - 0:00:48 - Epoch 0, iter 75, loss: 11.278133392333984
0: INFO    25-01-19 22:42:09.034270 - 0:00:49 - Epoch 0, iter 76, loss: 11.172168731689453
0: INFO    25-01-19 22:42:09.573285 - 0:00:49 - Epoch 0, iter 77, loss: 11.352606773376465
0: INFO    25-01-19 22:42:10.096408 - 0:00:50 - Epoch 0, iter 78, loss: 13.145204544067383
0: INFO    25-01-19 22:42:10.644798 - 0:00:51 - Epoch 0, iter 79, loss: 19.45503044128418
0: INFO    25-01-19 22:42:11.190449 - 0:00:51 - Epoch 0, iter 80, loss: 11.37841510772705
0: INFO    25-01-19 22:42:11.729109 - 0:00:52 - Epoch 0, iter 81, loss: 11.46642780303955
0: INFO    25-01-19 22:42:12.273819 - 0:00:52 - Epoch 0, iter 82, loss: 11.736711502075195
0: INFO    25-01-19 22:42:12.821548 - 0:00:53 - Epoch 0, iter 83, loss: 14.586555480957031
0: INFO    25-01-19 22:42:13.354754 - 0:00:53 - Epoch 0, iter 84, loss: 11.823226928710938
0: INFO    25-01-19 22:42:13.903712 - 0:00:54 - Epoch 0, iter 85, loss: 12.027034759521484
0: INFO    25-01-19 22:42:14.433827 - 0:00:54 - Epoch 0, iter 86, loss: 12.298055648803711
0: INFO    25-01-19 22:42:14.973665 - 0:00:55 - Epoch 0, iter 87, loss: 11.741708755493164
0: INFO    25-01-19 22:42:15.525937 - 0:00:55 - Epoch 0, iter 88, loss: 11.805583953857422
0: INFO    25-01-19 22:42:16.067004 - 0:00:56 - Epoch 0, iter 89, loss: 12.007797241210938
0: INFO    25-01-19 22:42:16.605130 - 0:00:56 - Epoch 0, iter 90, loss: 11.802919387817383
0: INFO    25-01-19 22:42:17.157243 - 0:00:57 - Epoch 0, iter 91, loss: 11.446626663208008
0: INFO    25-01-19 22:42:17.688867 - 0:00:58 - Epoch 0, iter 92, loss: 11.942547798156738
Traceback (most recent call last):
  File "/home/fumiyauchiyama/anaconda3/envs/py10/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/fumiyauchiyama/anaconda3/envs/py10/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/fumiyauchiyama/repos/x-anything/scripts/main/train.py", line 33, in <module>
    main()
  File "/home/fumiyauchiyama/repos/x-anything/scripts/main/train.py", line 30, in main
    trainer.train()
  File "/home/fumiyauchiyama/repos/x-anything/scripts/main/trainer.py", line 368, in train
    self.train_one_epoch(epoch, step)
  File "/home/fumiyauchiyama/repos/x-anything/scripts/main/trainer.py", line 342, in train_one_epoch
    loss[CORE_LOSS_KEY].backward()
  File "/home/fumiyauchiyama/anaconda3/envs/py10/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/fumiyauchiyama/anaconda3/envs/py10/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/fumiyauchiyama/anaconda3/envs/py10/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/fumiyauchiyama/anaconda3/envs/py10/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank0]:     return _run_code(code, main_globals, None,
[rank0]:   File "/home/fumiyauchiyama/anaconda3/envs/py10/lib/python3.10/runpy.py", line 86, in _run_code
[rank0]:     exec(code, run_globals)
[rank0]:   File "/home/fumiyauchiyama/repos/x-anything/scripts/main/train.py", line 33, in <module>
[rank0]:     main()
[rank0]:   File "/home/fumiyauchiyama/repos/x-anything/scripts/main/train.py", line 30, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/fumiyauchiyama/repos/x-anything/scripts/main/trainer.py", line 368, in train
[rank0]:     self.train_one_epoch(epoch, step)
[rank0]:   File "/home/fumiyauchiyama/repos/x-anything/scripts/main/trainer.py", line 342, in train_one_epoch
[rank0]:     loss[CORE_LOSS_KEY].backward()
[rank0]:   File "/home/fumiyauchiyama/anaconda3/envs/py10/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/fumiyauchiyama/anaconda3/envs/py10/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/fumiyauchiyama/anaconda3/envs/py10/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: KeyboardInterrupt
